{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38ec7656",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "33: error: type mismatch;",
     "output_type": "error",
     "traceback": [
      "<console>:33: error: type mismatch;",
      " found   : org.apache.spark.sql.types.StructType",
      " required: Seq[String]",
      "       val edges_df = spark.createDataFrame(Seq((0, 1), (1, 2), (1, 3), (3, 4), (5, 6), (6, 7), (7, 8))).toDF(schema: _*)",
      "                                                                                                              ^",
      ""
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.{SparkSession, functions}\n",
    "import org.apache.spark.sql.types.{StructType, StructField, IntegerType}\n",
    "\n",
    "// create a SparkSession object\n",
    "val spark = SparkSession.builder.appName(\"GraphDF\").getOrCreate()\n",
    "\n",
    "// create a graph DataFrame\n",
    "val schema = new StructType()\n",
    "         .add(\"src\", IntegerType)\n",
    "         .add(\"dst\", IntegerType)\n",
    "val edges_df = spark.createDataFrame(Seq((0, 1), (1, 2), (1, 3), (3, 4), (5, 6), (6, 7), (7, 8))).toDF(schema: _*)\n",
    "\n",
    "// print the DataFrame\n",
    "edges_df.show()\n",
    "\n",
    "// Function for the reducer\n",
    "def reducer(key: Int, values: Iterable[Int]): (Iterable[(Int, Int)], Int) = {\n",
    "    var minim = key\n",
    "    var valueList = List.empty[Int]\n",
    "    var to_emit = List.empty[(Int, Int)]\n",
    "    var counter = 0\n",
    "    \n",
    "    for (value <- values) {\n",
    "        if (value < minim) {\n",
    "            minim = value\n",
    "        }\n",
    "        valueList = valueList :+ value\n",
    "    }\n",
    "    \n",
    "    if (minim < key) {\n",
    "        to_emit = to_emit :+ (key, minim)\n",
    "        for (value <- valueList) {\n",
    "            if (minim != value) {\n",
    "                counter += 1\n",
    "                to_emit = to_emit :+ (value, minim)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    (to_emit, counter)\n",
    "}\n",
    "\n",
    "// initialising count_new_pairs with a value > 0 to run the first iteration\n",
    "val start_time = System.currentTimeMillis()\n",
    "var count_new_pairs = 1\n",
    "var iterations = 0\n",
    "\n",
    "// Loop for the iterations\n",
    "while (count_new_pairs > 0) {\n",
    "    // CCF iterate\n",
    "    // Map\n",
    "    val result_map = edges_df.select(functions.col(\"src\").alias(\"key\"), functions.col(\"dst\").alias(\"value\")).union(\n",
    "        edges_df.select(functions.col(\"dst\").alias(\"key\"), functions.col(\"src\").alias(\"value\")))\n",
    "    // Shuffle and Sort\n",
    "    val result_shufflesort = result_map.groupBy(\"key\").agg(functions.collect_list(\"value\").alias(\"values\"))\n",
    "    // Reduce\n",
    "    val result_reducer = result_shufflesort.rdd.map(x => reducer(x.getAs[Int](\"key\"), x.getAs[Seq[Int]](\"values\")))\n",
    "    val result_reducer2 = result_reducer.flatMap(x => x._1).toDF(\"src\", \"dst\")\n",
    "    // Updating the counting\n",
    "    count_new_pairs = result_reducer.map(x => x._2).reduce(_ + _)\n",
    "    // CCF dedup\n",
    "    edges_df = result_reducer2.dropDuplicates()\n",
    "    iterations += 1\n",
    "}\n",
    "\n",
    "val sorted_df = edges_df.sort(functions.col(\"dst\"))\n",
    "val elapsed_time = System.currentTimeMillis() - start_time\n",
    "\n",
    "sorted_df.show()\n",
    "println(\"Iterations:\", iterations)\n",
    "println(\"Elapsed time: \", elapsed_time)\n",
    "\n",
    "// stop the SparkSession object\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
